{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "pd.set_option('display.max_row', 1000)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pickle\n",
    "data = pd.read_csv(\"./data/kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data From Pickel\n",
    "with open('regression_Initial_Model_sk.pickle', 'rb') as file:\n",
    "    data_Model1_sk = pickle.load(file)\n",
    "    \n",
    "with open('scaler_1.pickle', 'rb') as file:\n",
    "    scaler_1 = pickle.load(file)    \n",
    "\n",
    "with open('data_Target_Homes_Not_Scaled_WithDummies.pickle', 'rb') as file:\n",
    "    data_Target_Homes_Not_Scaled_WithDummies = pickle.load(file)    \n",
    "\n",
    "#need to create new pickle file of variables that we start with when we scale vs. the end\n",
    "# with open('model1_Continuous_variables.pickle', 'rb') as file:\n",
    "#     model1_Continuous_variables = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the columns to scale, ensure alignment, need different names given different dataframes and prefixes for each type of the same col\n",
    "colsToScale = ['bedrooms', 'bathrooms','sqft_living','sqft_lot','sqft_basement','sqft_living15', 'sqft_lot15', 'floors',\n",
    "               'sqft_above','AgeOfHome','ttl_rooms']\n",
    "\n",
    "colsToScale_with_a = list([\"a_\" + e for e in colsToScale])\n",
    "colsToScale_with_sc = list([\"sc_\" + e for e in colsToScale])\n",
    "colsToScale_with_n_sc  = list([\"n_sc_\" + e for e in colsToScale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data in pickel file, required given the need to import data in an unscaled state.  Only way change the data using \n",
    "#Unscaled numbers\n",
    "\n",
    "data_Target_Homes_colsToScale = data_Target_Homes_Not_Scaled_WithDummies[colsToScale] \n",
    "\n",
    "Scaled_Data_1 = scaler_1.transform(data_Target_Homes_colsToScale)\n",
    "df_Scaled_Data_1 = pd.DataFrame(data=Scaled_Data_1, columns=colsToScale_with_sc)\n",
    "\n",
    "#Combined scaled values with original values\n",
    "data_Target_Homes_Not_Scaled_WithDummies = data_Target_Homes_Not_Scaled_WithDummies.reset_index(drop=True)\n",
    "data_Target_Homes_Scaled_WithDummies = pd.concat([data_Target_Homes_Not_Scaled_WithDummies, df_Scaled_Data_1], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get and insert the average value for each feature for easy retravial when adding values to demonstrate regression\n",
    "def resetAverages():\n",
    "    df_Averages = pd.DataFrame()\n",
    "    df_Averages['a_bedrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bedrooms'].mean()]\n",
    "    df_Averages['a_bathrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bathrooms'].mean()]\n",
    "    df_Averages['a_sqft_living'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living'].mean()]\n",
    "    df_Averages['a_sqft_lot'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot'].mean()]\n",
    "    df_Averages['a_sqft_basement'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_basement'].mean()]\n",
    "    df_Averages['a_sqft_living15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living15'].mean()]\n",
    "    df_Averages['a_sqft_lot15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot15'].mean()]\n",
    "    df_Averages['a_floors'] = [data_Target_Homes_Not_Scaled_WithDummies['floors'].mean()]\n",
    "    df_Averages['a_sqft_above'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_above'].mean()]\n",
    "    df_Averages['a_AgeOfHome'] = [data_Target_Homes_Not_Scaled_WithDummies['AgeOfHome'].mean()]\n",
    "    df_Averages['a_ttl_rooms'] = [data_Target_Homes_Not_Scaled_WithDummies['ttl_rooms'].mean()]\n",
    "    df_Averages['a_waterfront_1'] = [0]\n",
    "    df_Averages['a_view_1'] = [0]\n",
    "    df_Averages['a_view_2'] = [0]\n",
    "    df_Averages['a_view_3'] = [0]\n",
    "    df_Averages['a_view_4'] = [0]\n",
    "    df_Averages['a_condition_4'] = [0]\n",
    "    df_Averages['a_condition_5'] = [0]\n",
    "    df_Averages['a_grade_6'] = [0]\n",
    "    df_Averages['a_grade_8'] = [0]\n",
    "    df_Averages['a_grade_9'] = [0]\n",
    "    df_Averages['a_grade_10'] = [0]\n",
    "    df_Averages['a_grade_11'] = [0]\n",
    "    df_Averages['a_ZipFirst3_981'] =[0]\n",
    "    df_Averages['a_sls_mnth_2'] = [0]\n",
    "    df_Averages['a_sls_mnth_3'] = [0]\n",
    "    df_Averages['a_sls_mnth_4'] = [0]\n",
    "    df_Averages['a_sls_mnth_5'] = [1]\n",
    "    df_Averages['a_sls_mnth_6'] = [0]\n",
    "    df_Averages['a_sls_mnth_7'] = [0]\n",
    "    df_Averages['a_sls_mnth_8'] = [0]\n",
    "    df_Averages['a_sls_mnth_9'] = [0]\n",
    "    df_Averages['a_sls_mnth_10'] = [0]\n",
    "    df_Averages['a_sls_mnth_11'] = [0]\n",
    "    df_Averages['a_sls_mnth_12'] = [0]\n",
    "    df_Averages['a_If_renovated_1'] = [0]\n",
    "    \n",
    "    # caluculate the averages after scaling the dataframe\n",
    "    df_Averages['a_sc_bedrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bedrooms'].mean()\n",
    "    df_Averages['a_sc_bathrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bathrooms'].mean()\n",
    "    df_Averages['a_sc_sqft_living'] =data_Target_Homes_Scaled_WithDummies['sc_sqft_living'].mean()\n",
    "    df_Averages['a_sc_sqft_lot'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot'].mean()\n",
    "    df_Averages['a_sc_sqft_basement'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_basement'].mean()\n",
    "    df_Averages['a_sc_sqft_living15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_living15'].mean()\n",
    "    df_Averages['a_sc_sqft_lot15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot15'].mean()\n",
    "    df_Averages['a_sc_floors'] = data_Target_Homes_Scaled_WithDummies['sc_floors'].mean()\n",
    "    df_Averages['a_sc_sqft_above'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_above'].mean()\n",
    "    df_Averages['a_sc_AgeOfHome'] = data_Target_Homes_Scaled_WithDummies['sc_AgeOfHome'].mean()\n",
    "    df_Averages['a_sc_ttl_rooms'] = data_Target_Homes_Scaled_WithDummies['sc_ttl_rooms'].mean()\n",
    "    return df_Averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update value of target column using unscaled units\n",
    "def Calculate_New_Unscaled_Value_For_TargetCol(TargetColumn, deltaInValue, TargetDataFrame, id=0):\n",
    "    \n",
    "    #if continueous/ squart foot above \n",
    "    if (TargetColumn == \"sqft_above\") or (TargetColumn == \"a_sqft_above\"):\n",
    "        Target_UnscaledValue_ForTargetCol = TargetDataFrame[TargetColumn]\n",
    "        UpdatedValue_ForTargetCol = Target_UnscaledValue_ForTargetCol + deltaInValue\n",
    "        \n",
    "    if (TargetColumn[:5] == \"grade\") or (TargetColumn[:7] == \"a_grade\"):\n",
    "        Target_UnscaledValue_ForTargetCol = TargetDataFrame[TargetColumn]\n",
    "        UpdatedValue_ForTargetCol = deltaInValue\n",
    "        \n",
    "    return UpdatedValue_ForTargetCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.87795416]\n",
      "[391583.54230548]\n"
     ]
    }
   ],
   "source": [
    "#Scenario 1, using Model 1, Physical Features Only\n",
    "\n",
    "\n",
    "#Start by getting The \"Avearge\" home \n",
    "df_Averages = resetAverages()\n",
    "\n",
    "#Scenario 1, Impact of Increasing square Footage Above\n",
    "# TargetColumn = \"sqft_above\"\n",
    "# targetCol = \"a_\" + TargetColumn\n",
    "\n",
    "ListOfTargets = [\"sqft_above\"]\n",
    "ListOfdeltas = [100]\n",
    "\n",
    "# ListOfTargets = [\"grade_10\"]\n",
    "# ListOfdeltas = [1]\n",
    "\n",
    "# ListOfTargets = [\"sqft_above\", \"grade_8\"]\n",
    "# ListOfdeltas = [0,1]\n",
    "\n",
    "ChangedFeatures = list(zip(ListOfTargets, ListOfdeltas))\n",
    "\n",
    "#assume 100 additional square feet\n",
    "# deltaInValue = 100\n",
    "\n",
    "#enter In targeted home, or enter 0 to get the average change\n",
    "# parcelID = 6414100192\n",
    "parcelID = 0\n",
    "\n",
    "if parcelID ==0:\n",
    "    #we are calculating for an avearge\n",
    "    df_target_data_to_scale = df_Averages\n",
    "    \n",
    "    for targetCol, delta in ChangedFeatures:\n",
    "        TargetColumn = \"a_\" + targetCol\n",
    "        NewValue = Calculate_New_Unscaled_Value_For_TargetCol(TargetColumn,delta,df_target_data_to_scale,parcelID)\n",
    "        df_target_data_to_scale[TargetColumn] = NewValue\n",
    "        \n",
    "        #if changing a dummy variable need to reset all dummies along with updating the new dummy column\n",
    "        \n",
    "        if (TargetColumn[:7] == \"a_grade\"):\n",
    "            df_target_data_to_scale[\"a_grade_11\"] = 0\n",
    "            df_target_data_to_scale[\"grade_10\"] = 0\n",
    "            df_target_data_to_scale[\"a_grade_9\"] = 0\n",
    "            df_target_data_to_scale[\"a_grade_8\"] = 0\n",
    "            df_target_data_to_scale[\"a_grade_6\"] = 0\n",
    "            df_target_data_to_scale[TargetColumn] = 1\n",
    "            \n",
    "    #Need to changed colstoscale given target dataframe has cols with \"a_\" \n",
    "    colsToScale = colsToScale_with_a\n",
    "    \n",
    "    colsToFeed_In_to_Predict = ['a_waterfront_1', 'a_view_1', 'a_view_2', 'a_view_3', 'a_view_4', 'a_condition_4',\n",
    "       'a_condition_5', 'a_grade_6', 'a_grade_8', 'a_grade_9', 'a_grade_10', 'a_grade_11',\n",
    "       'a_ZipFirst3_981', 'a_sls_mnth_2', 'a_sls_mnth_3', 'a_sls_mnth_4', 'a_sls_mnth_5',\n",
    "       'a_sls_mnth_6', 'a_sls_mnth_7', 'a_sls_mnth_8', 'a_sls_mnth_9', 'a_sls_mnth_10',\n",
    "       'a_sls_mnth_11', 'a_sls_mnth_12', 'a_If_renovated_1', 'n_sc_sqft_lot',\n",
    "       'n_sc_sqft_basement', 'n_sc_sqft_above']\n",
    "\n",
    "\n",
    "else:\n",
    "    # we are calculating data using a real home\n",
    "    df_target_data_to_scale = data_Target_Homes_Not_Scaled_WithDummies.loc[data_Target_Homes_Not_Scaled_WithDummies['id']==parcelID]\n",
    "    \n",
    "    for targetCol, delta in ChangedFeatures:\n",
    "        TargetColumn = targetCol\n",
    "        NewValue = Calculate_New_Unscaled_Value_For_TargetCol(TargetColumn,delta,df_target_data_to_scale,parcelID)\n",
    "        df_target_data_to_scale[TargetColumn] = NewValue\n",
    "        \n",
    "        if (TargetColumn[:5] == \"grade\"):\n",
    "            df_target_data_to_scale[\"grade_11\"] = 0\n",
    "            df_target_data_to_scale[\"grade_10\"] = 0\n",
    "            df_target_data_to_scale[\"grade_9\"] = 0\n",
    "            df_target_data_to_scale[\"grade_8\"] = 0\n",
    "            df_target_data_to_scale[\"grade_6\"] = 0\n",
    "#             df_target_data_to_scale[TargetColumn] = 1\n",
    "\n",
    "    colsToFeed_In_to_Predict = ['waterfront_1', 'view_1', 'view_2', 'view_3', 'view_4', 'condition_4',\n",
    "       'condition_5', 'grade_6', 'grade_8', 'grade_9', 'grade_10', 'grade_11',\n",
    "       'ZipFirst3_981', 'sls_mnth_2', 'sls_mnth_3', 'sls_mnth_4', 'sls_mnth_5',\n",
    "       'sls_mnth_6', 'sls_mnth_7', 'sls_mnth_8', 'sls_mnth_9', 'sls_mnth_10',\n",
    "       'sls_mnth_11', 'sls_mnth_12', 'If_renovated_1', 'n_sc_sqft_lot',\n",
    "       'n_sc_sqft_basement', 'n_sc_sqft_above']\n",
    "\n",
    "#Scale and create dataframe of scaled values\n",
    "Scaled_Data_1 = scaler_1.transform(df_target_data_to_scale[colsToScale])\n",
    "df_Scaled_Data_1 = pd.DataFrame(data=Scaled_Data_1, columns=colsToScale_with_n_sc)\n",
    "\n",
    "#combined scaled values with other non scaled features needed to feed into predcit\n",
    "df_target_data_to_scale = df_target_data_to_scale.reset_index(drop=True)\n",
    "data_TargetHomes_With_Updated_TargetColumn = pd.concat([df_target_data_to_scale, df_Scaled_Data_1], axis=1)\n",
    "\n",
    "#Get the columns needed and order required to feed into predict\n",
    "df_Home_To_Predict_No_ID = data_TargetHomes_With_Updated_TargetColumn [colsToFeed_In_to_Predict]\n",
    "\n",
    "y_hat_average = data_Model1_sk.predict(df_Home_To_Predict_No_ID)\n",
    "print(y_hat_average)\n",
    "print(np.exp(y_hat_average))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Adding Location Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update school district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data From Pickel\n",
    "with open('regression_Second_Model_sk.pickle', 'rb') as file:\n",
    "    data_Model2_sk = pickle.load(file)\n",
    "    \n",
    "with open('scaler_2.pickle', 'rb') as file:\n",
    "    scaler_2 = pickle.load(file)    \n",
    "\n",
    "with open('data_with_Hot_Dist_POints_WithDummies.pickle', 'rb') as file:\n",
    "    data_with_Hot_Dist_POints_WithDummies = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('regression_Data_Second_Model.pickle', 'rb') as file:\n",
    "    data_Model2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get and insert the average value for each feature for easy retravial when adding values to demonstrate regression\n",
    "def resetAverages():\n",
    "    df_Averages = pd.DataFrame()\n",
    "    df_Averages['a_bedrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bedrooms'].mean()]\n",
    "    df_Averages['a_bathrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bathrooms'].mean()]\n",
    "    df_Averages['a_sqft_living'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living'].mean()]\n",
    "    df_Averages['a_sqft_lot'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot'].mean()]\n",
    "    df_Averages['a_sqft_basement'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_basement'].mean()]\n",
    "    df_Averages['a_sqft_living15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living15'].mean()]\n",
    "    df_Averages['a_sqft_lot15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot15'].mean()]\n",
    "    df_Averages['a_floors'] = [data_Target_Homes_Not_Scaled_WithDummies['floors'].mean()]\n",
    "    df_Averages['a_sqft_above'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_above'].mean()]\n",
    "    df_Averages['a_AgeOfHome'] = [data_Target_Homes_Not_Scaled_WithDummies['AgeOfHome'].mean()]\n",
    "    df_Averages['a_ttl_rooms'] = [data_Target_Homes_Not_Scaled_WithDummies['ttl_rooms'].mean()]\n",
    "    df_Averages['a_waterfront_1'] = [0]\n",
    "    df_Averages['a_view_1'] = [0]\n",
    "    df_Averages['a_view_2'] = [0]\n",
    "    df_Averages['a_view_3'] = [0]\n",
    "    df_Averages['a_view_4'] = [0]\n",
    "    df_Averages['a_condition_4'] = [0]\n",
    "    df_Averages['a_condition_5'] = [0]\n",
    "    df_Averages['a_grade_6'] = [0]\n",
    "    df_Averages['a_grade_8'] = [0]\n",
    "    df_Averages['a_grade_9'] = [0]\n",
    "    df_Averages['a_grade_10'] = [0]\n",
    "    df_Averages['a_grade_11'] = [0]\n",
    "    df_Averages['a_ZipFirst3_981'] =[0]\n",
    "    df_Averages['a_sls_mnth_2'] = [0]\n",
    "    df_Averages['a_sls_mnth_3'] = [0]\n",
    "    df_Averages['a_sls_mnth_4'] = [0]\n",
    "    df_Averages['a_sls_mnth_5'] = [1]\n",
    "    df_Averages['a_sls_mnth_6'] = [0]\n",
    "    df_Averages['a_sls_mnth_7'] = [0]\n",
    "    df_Averages['a_sls_mnth_8'] = [0]\n",
    "    df_Averages['a_sls_mnth_9'] = [0]\n",
    "    df_Averages['a_sls_mnth_10'] = [0]\n",
    "    df_Averages['a_sls_mnth_11'] = [0]\n",
    "    df_Averages['a_sls_mnth_12'] = [0]\n",
    "    df_Averages['a_If_renovated_1'] = [0]\n",
    "    \n",
    "    # caluculate the averages after scaling the dataframe\n",
    "    df_Averages['a_sc_bedrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bedrooms'].mean()\n",
    "    df_Averages['a_sc_bathrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bathrooms'].mean()\n",
    "    df_Averages['a_sc_sqft_living'] =data_Target_Homes_Scaled_WithDummies['sc_sqft_living'].mean()\n",
    "    df_Averages['a_sc_sqft_lot'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot'].mean()\n",
    "    df_Averages['a_sc_sqft_basement'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_basement'].mean()\n",
    "    df_Averages['a_sc_sqft_living15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_living15'].mean()\n",
    "    df_Averages['a_sc_sqft_lot15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot15'].mean()\n",
    "    df_Averages['a_sc_floors'] = data_Target_Homes_Scaled_WithDummies['sc_floors'].mean()\n",
    "    df_Averages['a_sc_sqft_above'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_above'].mean()\n",
    "    df_Averages['a_sc_AgeOfHome'] = data_Target_Homes_Scaled_WithDummies['sc_AgeOfHome'].mean()\n",
    "    df_Averages['a_sc_ttl_rooms'] = data_Target_Homes_Scaled_WithDummies['sc_ttl_rooms'].mean()\n",
    "    \n",
    "    #newly added averages from datamodel 2\n",
    "    df_Averages['a_Sch_d_Top15'] = 1\n",
    "    df_Averages['a_Sch_d_Top30'] = 0\n",
    "    df_Averages['a_Sch_d_Top60'] = 0\n",
    "    df_Averages['a_Under10'] = 10\n",
    "    df_Averages['a_Over20'] = 10\n",
    "    df_Averages['a_sc_Under10'] = data_Model2[\"Under10\"].mean()\n",
    "    df_Averages['a_sc_Over20'] = data_Model2[\"Over20\"].mean()\n",
    "    \n",
    "    return df_Averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = resetAverages()\n",
    "testAveragePredict = test[['a_Sch_d_Top15', 'a_Sch_d_Top30', 'a_Sch_d_Top60', 'a_sc_Under10', 'a_sc_Over20',\n",
    "       'a_sc_sqft_lot', 'a_sc_sqft_basement', 'a_sc_sqft_above', 'a_view_1', 'a_view_2', 'a_view_3',\n",
    "       'a_view_4', 'a_condition_4', 'a_condition_5', 'a_grade_6', 'a_grade_8', 'a_grade_9',\n",
    "       'a_grade_10', 'a_sls_mnth_2', 'a_sls_mnth_3', 'a_sls_mnth_4', 'a_sls_mnth_5',\n",
    "       'a_sls_mnth_6', 'a_sls_mnth_7', 'a_sls_mnth_8', 'a_sls_mnth_9', 'a_sls_mnth_10',\n",
    "       'a_sls_mnth_11', 'a_sls_mnth_12', 'a_If_renovated_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  3 - Adding Assesor Appraisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns To feed into scaler\n",
    "# model3_Continuous_variables  = ['YrRollingAppraisal','AppraisedLandValue', \n",
    "#                                   'AppraisedImpsValue', 'LandToHouseCheck',\n",
    "#                                   'InflationSinceLastAppraisal', \n",
    "#                                   'InflationSinceLastAppraisal_abs', 'AppraisedTotal', 'AssesorAppraisals_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data From Pickel\n",
    "with open('regression_Third_Model_sk.pickle', 'rb') as file:\n",
    "    data_Model3_sk = pickle.load(file)\n",
    "    \n",
    "with open('scaler_3.pickle', 'rb') as file:\n",
    "    scaler_2 = pickle.load(file)    \n",
    "\n",
    "with open('df_data_with_Appraisals.pickle', 'rb') as file:\n",
    "    df_data_with_Appraisals = pickle.load(file)\n",
    "    \n",
    "    \n",
    "with open('regression_Data_Third_Model.pickle', 'rb') as file:\n",
    "    data_Model3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get and insert the average value for each feature for easy retravial when adding values to demonstrate regression\n",
    "def resetAverages():\n",
    "    df_Averages = pd.DataFrame()\n",
    "    df_Averages['a_bedrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bedrooms'].mean()]\n",
    "    df_Averages['a_bathrooms'] = [data_Target_Homes_Not_Scaled_WithDummies['bathrooms'].mean()]\n",
    "    df_Averages['a_sqft_living'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living'].mean()]\n",
    "    df_Averages['a_sqft_lot'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot'].mean()]\n",
    "    df_Averages['a_sqft_basement'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_basement'].mean()]\n",
    "    df_Averages['a_sqft_living15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_living15'].mean()]\n",
    "    df_Averages['a_sqft_lot15'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_lot15'].mean()]\n",
    "    df_Averages['a_floors'] = [data_Target_Homes_Not_Scaled_WithDummies['floors'].mean()]\n",
    "    df_Averages['a_sqft_above'] = [data_Target_Homes_Not_Scaled_WithDummies['sqft_above'].mean()]\n",
    "    df_Averages['a_AgeOfHome'] = [data_Target_Homes_Not_Scaled_WithDummies['AgeOfHome'].mean()]\n",
    "    df_Averages['a_ttl_rooms'] = [data_Target_Homes_Not_Scaled_WithDummies['ttl_rooms'].mean()]\n",
    "    df_Averages['a_waterfront_1'] = [0]\n",
    "    df_Averages['a_view_1'] = [0]\n",
    "    df_Averages['a_view_2'] = [0]\n",
    "    df_Averages['a_view_3'] = [0]\n",
    "    df_Averages['a_view_4'] = [0]\n",
    "    df_Averages['a_condition_4'] = [0]\n",
    "    df_Averages['a_condition_5'] = [0]\n",
    "    df_Averages['a_grade_6'] = [0]\n",
    "    df_Averages['a_grade_8'] = [0]\n",
    "    df_Averages['a_grade_9'] = [0]\n",
    "    df_Averages['a_grade_10'] = [0]\n",
    "    df_Averages['a_grade_11'] = [0]\n",
    "    df_Averages['a_ZipFirst3_981'] =[0]\n",
    "    df_Averages['a_sls_mnth_2'] = [0]\n",
    "    df_Averages['a_sls_mnth_3'] = [0]\n",
    "    df_Averages['a_sls_mnth_4'] = [0]\n",
    "    df_Averages['a_sls_mnth_5'] = [1]\n",
    "    df_Averages['a_sls_mnth_6'] = [0]\n",
    "    df_Averages['a_sls_mnth_7'] = [0]\n",
    "    df_Averages['a_sls_mnth_8'] = [0]\n",
    "    df_Averages['a_sls_mnth_9'] = [0]\n",
    "    df_Averages['a_sls_mnth_10'] = [0]\n",
    "    df_Averages['a_sls_mnth_11'] = [0]\n",
    "    df_Averages['a_sls_mnth_12'] = [0]\n",
    "    df_Averages['a_If_renovated_1'] = [0]\n",
    "    \n",
    "    # caluculate the averages after scaling the dataframe\n",
    "    df_Averages['a_sc_bedrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bedrooms'].mean()\n",
    "    df_Averages['a_sc_bathrooms'] = data_Target_Homes_Scaled_WithDummies['sc_bathrooms'].mean()\n",
    "    df_Averages['a_sc_sqft_living'] =data_Target_Homes_Scaled_WithDummies['sc_sqft_living'].mean()\n",
    "    df_Averages['a_sc_sqft_lot'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot'].mean()\n",
    "    df_Averages['a_sc_sqft_basement'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_basement'].mean()\n",
    "    df_Averages['a_sc_sqft_living15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_living15'].mean()\n",
    "    df_Averages['a_sc_sqft_lot15'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_lot15'].mean()\n",
    "    df_Averages['a_sc_floors'] = data_Target_Homes_Scaled_WithDummies['sc_floors'].mean()\n",
    "    df_Averages['a_sc_sqft_above'] = data_Target_Homes_Scaled_WithDummies['sc_sqft_above'].mean()\n",
    "    df_Averages['a_sc_AgeOfHome'] = data_Target_Homes_Scaled_WithDummies['sc_AgeOfHome'].mean()\n",
    "    df_Averages['a_sc_ttl_rooms'] = data_Target_Homes_Scaled_WithDummies['sc_ttl_rooms'].mean()\n",
    "    \n",
    "    #newly added averages from datamodel 2\n",
    "    df_Averages['a_Sch_d_Top15'] = 1\n",
    "    df_Averages['a_Sch_d_Top30'] = 0\n",
    "    df_Averages['a_Sch_d_Top60'] = 0\n",
    "#     df_Averages['a_Under10'] = 10\n",
    "#     df_Averages['a_Over20'] = 10\n",
    "    df_Averages['a_sc_Under10'] = data_Model2[\"Under10\"].mean()\n",
    "    df_Averages['a_sc_Over20'] = data_Model2[\"Over20\"].mean()\n",
    "    \n",
    "    #newly added averages from datamodel 3\n",
    "#     df_Averages['YrRollingAppraisal'] \n",
    "#     df_Averages['AppraisedLandValue']\n",
    "#     df_Averages['AppraisedImpsValue']\n",
    "#     df_Averages['LandToHouseCheck']\n",
    "#     df_Averages['InflationSinceLastAppraisal']\n",
    "#     df_Averages['InflationSinceLastAppraisal_abs']\n",
    "#     df_Averages['AppraisedTotal']\n",
    "    df_Averages['a_sc_AssesorAppraisals_x'] = df_data_with_Appraisals['AssesorAppraisals_x'].mean()\n",
    "        \n",
    "    \n",
    "    \n",
    "    return df_Averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average = resetAverages()\n",
    "testAveragePredict = df_average[['a_sc_AssesorAppraisals_x', 'a_Sch_d_Top15', 'a_Sch_d_Top30', 'a_Sch_d_Top60',\n",
    "       'a_sc_Under10', 'a_sc_Over20', 'a_view_1', 'a_view_2', 'a_view_3', 'a_view_4',\n",
    "       'a_condition_4', 'a_condition_5', 'a_grade_6', 'a_grade_8', 'a_grade_9',\n",
    "       'a_grade_10', 'a_sls_mnth_2', 'a_sls_mnth_3', 'a_sls_mnth_4', 'a_sls_mnth_5',\n",
    "       'a_sls_mnth_6', 'a_sls_mnth_7', 'a_sls_mnth_8', 'a_sls_mnth_9', 'a_sls_mnth_10',\n",
    "       'a_sls_mnth_11', 'a_sls_mnth_12', 'a_If_renovated_1', 'a_sc_sqft_lot',\n",
    "       'a_sc_sqft_basement', 'a_sc_sqft_above']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.03987984]\n",
      "[460413.29988845]\n"
     ]
    }
   ],
   "source": [
    "y_hat_average = data_Model3_sk.predict(testAveragePredict)\n",
    "print(y_hat_average)\n",
    "print(np.exp(y_hat_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sch_d_Top15</th>\n",
       "      <th>Sch_d_Top30</th>\n",
       "      <th>Sch_d_Top60</th>\n",
       "      <th>Under10</th>\n",
       "      <th>Over20</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>view_1</th>\n",
       "      <th>view_2</th>\n",
       "      <th>view_3</th>\n",
       "      <th>view_4</th>\n",
       "      <th>condition_4</th>\n",
       "      <th>condition_5</th>\n",
       "      <th>grade_6</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_9</th>\n",
       "      <th>grade_10</th>\n",
       "      <th>sls_mnth_2</th>\n",
       "      <th>sls_mnth_3</th>\n",
       "      <th>sls_mnth_4</th>\n",
       "      <th>sls_mnth_5</th>\n",
       "      <th>sls_mnth_6</th>\n",
       "      <th>sls_mnth_7</th>\n",
       "      <th>sls_mnth_8</th>\n",
       "      <th>sls_mnth_9</th>\n",
       "      <th>sls_mnth_10</th>\n",
       "      <th>sls_mnth_11</th>\n",
       "      <th>sls_mnth_12</th>\n",
       "      <th>If_renovated_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.540333</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>0.528125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sch_d_Top15  Sch_d_Top30  Sch_d_Top60   Under10  Over20  sqft_lot  sqft_basement  sqft_above  view_1  view_2  view_3  view_4  condition_4  condition_5  grade_6  grade_8  grade_9  grade_10  sls_mnth_2  sls_mnth_3  sls_mnth_4  sls_mnth_5  sls_mnth_6  sls_mnth_7  sls_mnth_8  sls_mnth_9  sls_mnth_10  sls_mnth_11  sls_mnth_12  If_renovated_1\n",
       "0            0            1            0  0.428571    0.05  0.540333       0.184332    0.528125       0       0       0       0            0            0        0        0        0         0           0           0           0           0           0           0           0           0            0            0            1               1"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPredictDatatop = testPredictData.head(1)\n",
    "testPredictDatatop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.28233237]\n",
      "[586737.24232559]\n"
     ]
    }
   ],
   "source": [
    "y_hat_average = data_Model2_sk.predict(testPredictDatatop)\n",
    "print(y_hat_average)\n",
    "print(np.exp(y_hat_average))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
